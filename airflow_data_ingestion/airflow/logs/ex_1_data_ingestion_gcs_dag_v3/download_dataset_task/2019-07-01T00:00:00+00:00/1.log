[2022-02-02 18:22:10,669] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ex_1_data_ingestion_gcs_dag_v3.download_dataset_task scheduled__2019-07-01T00:00:00+00:00 [queued]>
[2022-02-02 18:22:10,683] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ex_1_data_ingestion_gcs_dag_v3.download_dataset_task scheduled__2019-07-01T00:00:00+00:00 [queued]>
[2022-02-02 18:22:10,684] {taskinstance.py:1238} INFO - 
--------------------------------------------------------------------------------
[2022-02-02 18:22:10,685] {taskinstance.py:1239} INFO - Starting attempt 1 of 1
[2022-02-02 18:22:10,686] {taskinstance.py:1240} INFO - 
--------------------------------------------------------------------------------
[2022-02-02 18:22:10,700] {taskinstance.py:1259} INFO - Executing <Task(BashOperator): download_dataset_task> on 2019-07-01 00:00:00+00:00
[2022-02-02 18:22:10,708] {standard_task_runner.py:52} INFO - Started process 9087 to run task
[2022-02-02 18:22:10,713] {standard_task_runner.py:76} INFO - Running: ['***', 'tasks', 'run', 'ex_1_data_ingestion_gcs_dag_v3', 'download_dataset_task', 'scheduled__2019-07-01T00:00:00+00:00', '--job-id', '322', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion_hw_1_ex.py', '--cfg-path', '/tmp/tmp28pe0ae8', '--error-file', '/tmp/tmpfe772_fz']
[2022-02-02 18:22:10,715] {standard_task_runner.py:77} INFO - Job 322: Subtask download_dataset_task
[2022-02-02 18:22:10,805] {logging_mixin.py:109} INFO - Running <TaskInstance: ex_1_data_ingestion_gcs_dag_v3.download_dataset_task scheduled__2019-07-01T00:00:00+00:00 [running]> on host 700aeb16bae0
[2022-02-02 18:22:10,856] {warnings.py:110} WARNING - /home/***/.local/lib/python3.7/site-packages/***/utils/context.py:152: AirflowContextDeprecationWarning: Accessing 'execution_date' from the template is deprecated and will be removed in a future version. Please use 'data_interval_start' or 'logical_date' instead.
  warnings.warn(_create_deprecation_warning(key, self._deprecation_replacements[key]))

[2022-02-02 18:22:10,887] {taskinstance.py:1426} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=ex_1_data_ingestion_gcs_dag_v3
AIRFLOW_CTX_TASK_ID=download_dataset_task
AIRFLOW_CTX_EXECUTION_DATE=2019-07-01T00:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2019-07-01T00:00:00+00:00
[2022-02-02 18:22:10,889] {subprocess.py:62} INFO - Tmp dir root location: 
 /tmp
[2022-02-02 18:22:10,891] {subprocess.py:74} INFO - Running command: ['bash', '-c', 'curl -sS https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2019-07.csv > /opt/***/yellow_tripdata_2019-07.csv']
[2022-02-02 18:22:10,908] {subprocess.py:85} INFO - Output:
[2022-02-02 18:22:48,157] {subprocess.py:93} INFO - Command exited with return code 0
[2022-02-02 18:22:48,244] {taskinstance.py:1277} INFO - Marking task as SUCCESS. dag_id=ex_1_data_ingestion_gcs_dag_v3, task_id=download_dataset_task, execution_date=20190701T000000, start_date=20220202T182210, end_date=20220202T182248
[2022-02-02 18:22:48,286] {local_task_job.py:154} INFO - Task exited with return code 0
[2022-02-02 18:22:48,325] {local_task_job.py:264} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2022-02-10 12:58:00,761] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ex_1_data_ingestion_gcs_dag_v3.download_dataset_task scheduled__2019-07-01T00:00:00+00:00 [queued]>
[2022-02-10 12:58:00,775] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ex_1_data_ingestion_gcs_dag_v3.download_dataset_task scheduled__2019-07-01T00:00:00+00:00 [queued]>
[2022-02-10 12:58:00,776] {taskinstance.py:1238} INFO - 
--------------------------------------------------------------------------------
[2022-02-10 12:58:00,777] {taskinstance.py:1239} INFO - Starting attempt 1 of 1
[2022-02-10 12:58:00,778] {taskinstance.py:1240} INFO - 
--------------------------------------------------------------------------------
[2022-02-10 12:58:00,795] {taskinstance.py:1259} INFO - Executing <Task(BashOperator): download_dataset_task> on 2019-07-01 00:00:00+00:00
[2022-02-10 12:58:00,802] {standard_task_runner.py:52} INFO - Started process 1232 to run task
[2022-02-10 12:58:00,812] {standard_task_runner.py:76} INFO - Running: ['***', 'tasks', 'run', 'ex_1_data_ingestion_gcs_dag_v3', 'download_dataset_task', 'scheduled__2019-07-01T00:00:00+00:00', '--job-id', '678', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion_hw_1_ex.py', '--cfg-path', '/tmp/tmpmhgu20q3', '--error-file', '/tmp/tmpzs_7a9u6']
[2022-02-10 12:58:00,814] {standard_task_runner.py:77} INFO - Job 678: Subtask download_dataset_task
[2022-02-10 12:58:00,903] {logging_mixin.py:109} INFO - Running <TaskInstance: ex_1_data_ingestion_gcs_dag_v3.download_dataset_task scheduled__2019-07-01T00:00:00+00:00 [running]> on host e30f201fae61
[2022-02-10 12:58:00,950] {warnings.py:110} WARNING - /home/***/.local/lib/python3.7/site-packages/***/utils/context.py:152: AirflowContextDeprecationWarning: Accessing 'execution_date' from the template is deprecated and will be removed in a future version. Please use 'data_interval_start' or 'logical_date' instead.
  warnings.warn(_create_deprecation_warning(key, self._deprecation_replacements[key]))

[2022-02-10 12:58:00,979] {taskinstance.py:1426} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=ex_1_data_ingestion_gcs_dag_v3
AIRFLOW_CTX_TASK_ID=download_dataset_task
AIRFLOW_CTX_EXECUTION_DATE=2019-07-01T00:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2019-07-01T00:00:00+00:00
[2022-02-10 12:58:00,981] {subprocess.py:62} INFO - Tmp dir root location: 
 /tmp
[2022-02-10 12:58:00,983] {subprocess.py:74} INFO - Running command: ['bash', '-c', 'curl -sS https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2019-07.csv > /opt/***/yellow_tripdata_2019-07.csv']
[2022-02-10 12:58:00,997] {subprocess.py:85} INFO - Output:
[2022-02-10 13:00:28,474] {subprocess.py:93} INFO - Command exited with return code 0
[2022-02-10 13:00:28,506] {taskinstance.py:1277} INFO - Marking task as SUCCESS. dag_id=ex_1_data_ingestion_gcs_dag_v3, task_id=download_dataset_task, execution_date=20190701T000000, start_date=20220210T125800, end_date=20220210T130028
[2022-02-10 13:00:28,561] {local_task_job.py:154} INFO - Task exited with return code 0
[2022-02-10 13:00:28,608] {local_task_job.py:264} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2022-02-10 15:34:43,217] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ex_1_data_ingestion_gcs_dag_v3.download_dataset_task scheduled__2019-07-01T00:00:00+00:00 [queued]>
[2022-02-10 15:34:43,234] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ex_1_data_ingestion_gcs_dag_v3.download_dataset_task scheduled__2019-07-01T00:00:00+00:00 [queued]>
[2022-02-10 15:34:43,236] {taskinstance.py:1238} INFO - 
--------------------------------------------------------------------------------
[2022-02-10 15:34:43,236] {taskinstance.py:1239} INFO - Starting attempt 1 of 1
[2022-02-10 15:34:43,249] {taskinstance.py:1240} INFO - 
--------------------------------------------------------------------------------
[2022-02-10 15:34:43,279] {taskinstance.py:1259} INFO - Executing <Task(BashOperator): download_dataset_task> on 2019-07-01 00:00:00+00:00
[2022-02-10 15:34:43,289] {standard_task_runner.py:52} INFO - Started process 405 to run task
[2022-02-10 15:34:43,295] {standard_task_runner.py:76} INFO - Running: ['***', 'tasks', 'run', 'ex_1_data_ingestion_gcs_dag_v3', 'download_dataset_task', 'scheduled__2019-07-01T00:00:00+00:00', '--job-id', '797', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion_hw_1_ex.py', '--cfg-path', '/tmp/tmpu9ssdsf8', '--error-file', '/tmp/tmpmih1a4pp']
[2022-02-10 15:34:43,300] {standard_task_runner.py:77} INFO - Job 797: Subtask download_dataset_task
[2022-02-10 15:34:43,437] {logging_mixin.py:109} INFO - Running <TaskInstance: ex_1_data_ingestion_gcs_dag_v3.download_dataset_task scheduled__2019-07-01T00:00:00+00:00 [running]> on host 276a0ad90304
[2022-02-10 15:34:43,520] {warnings.py:110} WARNING - /home/***/.local/lib/python3.7/site-packages/***/utils/context.py:152: AirflowContextDeprecationWarning: Accessing 'execution_date' from the template is deprecated and will be removed in a future version. Please use 'data_interval_start' or 'logical_date' instead.
  warnings.warn(_create_deprecation_warning(key, self._deprecation_replacements[key]))

[2022-02-10 15:34:43,568] {taskinstance.py:1426} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=ex_1_data_ingestion_gcs_dag_v3
AIRFLOW_CTX_TASK_ID=download_dataset_task
AIRFLOW_CTX_EXECUTION_DATE=2019-07-01T00:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2019-07-01T00:00:00+00:00
[2022-02-10 15:34:43,574] {subprocess.py:62} INFO - Tmp dir root location: 
 /tmp
[2022-02-10 15:34:43,578] {subprocess.py:74} INFO - Running command: ['bash', '-c', 'curl -sS https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2019-07.csv > /opt/***/yellow_tripdata_2019-07.csv']
[2022-02-10 15:34:43,604] {subprocess.py:85} INFO - Output:
[2022-02-10 15:35:08,970] {subprocess.py:93} INFO - Command exited with return code 0
[2022-02-10 15:35:09,070] {taskinstance.py:1277} INFO - Marking task as SUCCESS. dag_id=ex_1_data_ingestion_gcs_dag_v3, task_id=download_dataset_task, execution_date=20190701T000000, start_date=20220210T153443, end_date=20220210T153509
[2022-02-10 15:35:09,150] {local_task_job.py:154} INFO - Task exited with return code 0
[2022-02-10 15:35:09,280] {local_task_job.py:264} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2022-02-10 16:33:52,122] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ex_1_data_ingestion_gcs_dag_v3.download_dataset_task scheduled__2019-07-01T00:00:00+00:00 [queued]>
[2022-02-10 16:33:52,185] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ex_1_data_ingestion_gcs_dag_v3.download_dataset_task scheduled__2019-07-01T00:00:00+00:00 [queued]>
[2022-02-10 16:33:52,187] {taskinstance.py:1238} INFO - 
--------------------------------------------------------------------------------
[2022-02-10 16:33:52,189] {taskinstance.py:1239} INFO - Starting attempt 1 of 1
[2022-02-10 16:33:52,191] {taskinstance.py:1240} INFO - 
--------------------------------------------------------------------------------
[2022-02-10 16:33:52,234] {taskinstance.py:1259} INFO - Executing <Task(BashOperator): download_dataset_task> on 2019-07-01 00:00:00+00:00
[2022-02-10 16:33:52,249] {standard_task_runner.py:52} INFO - Started process 365 to run task
[2022-02-10 16:33:52,256] {standard_task_runner.py:76} INFO - Running: ['***', 'tasks', 'run', 'ex_1_data_ingestion_gcs_dag_v3', 'download_dataset_task', 'scheduled__2019-07-01T00:00:00+00:00', '--job-id', '938', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion_hw_1_ex.py', '--cfg-path', '/tmp/tmpbfwqk8xb', '--error-file', '/tmp/tmpkbhvvxmm']
[2022-02-10 16:33:52,259] {standard_task_runner.py:77} INFO - Job 938: Subtask download_dataset_task
[2022-02-10 16:33:52,369] {logging_mixin.py:109} INFO - Running <TaskInstance: ex_1_data_ingestion_gcs_dag_v3.download_dataset_task scheduled__2019-07-01T00:00:00+00:00 [running]> on host 106a34111faf
[2022-02-10 16:33:52,485] {warnings.py:110} WARNING - /home/***/.local/lib/python3.7/site-packages/***/utils/context.py:152: AirflowContextDeprecationWarning: Accessing 'execution_date' from the template is deprecated and will be removed in a future version. Please use 'data_interval_start' or 'logical_date' instead.
  warnings.warn(_create_deprecation_warning(key, self._deprecation_replacements[key]))

[2022-02-10 16:33:52,548] {taskinstance.py:1426} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=ex_1_data_ingestion_gcs_dag_v3
AIRFLOW_CTX_TASK_ID=download_dataset_task
AIRFLOW_CTX_EXECUTION_DATE=2019-07-01T00:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2019-07-01T00:00:00+00:00
[2022-02-10 16:33:52,551] {subprocess.py:62} INFO - Tmp dir root location: 
 /tmp
[2022-02-10 16:33:52,554] {subprocess.py:74} INFO - Running command: ['bash', '-c', 'curl -sS https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2019-07.csv > /opt/***/yellow_tripdata_2019-07.csv']
[2022-02-10 16:33:52,581] {subprocess.py:85} INFO - Output:
[2022-02-10 16:35:18,810] {subprocess.py:89} INFO - curl: (56) OpenSSL SSL_read: SSL_ERROR_SYSCALL, errno 104
[2022-02-10 16:35:18,813] {subprocess.py:93} INFO - Command exited with return code 56
[2022-02-10 16:35:18,855] {taskinstance.py:1700} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1329, in _run_raw_task
    self._execute_task_with_callbacks(context)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1455, in _execute_task_with_callbacks
    result = self._execute_task(context, self.task)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1511, in _execute_task
    result = execute_callable(context=context)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/bash.py", line 188, in execute
    f'Bash command failed. The command returned a non-zero exit code {result.exit_code}.'
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 56.
[2022-02-10 16:35:18,880] {taskinstance.py:1277} INFO - Marking task as FAILED. dag_id=ex_1_data_ingestion_gcs_dag_v3, task_id=download_dataset_task, execution_date=20190701T000000, start_date=20220210T163352, end_date=20220210T163518
[2022-02-10 16:35:18,912] {standard_task_runner.py:92} ERROR - Failed to execute job 938 for task download_dataset_task
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/task/task_runner/standard_task_runner.py", line 85, in _start_by_fork
    args.func(args, dag=self.dag)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/cli/cli_parser.py", line 48, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/cli.py", line 92, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/cli/commands/task_command.py", line 298, in task_run
    _run_task_by_selected_method(args, dag, ti)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/cli/commands/task_command.py", line 107, in _run_task_by_selected_method
    _run_raw_task(args, ti)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/cli/commands/task_command.py", line 184, in _run_raw_task
    error_file=args.error_file,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1329, in _run_raw_task
    self._execute_task_with_callbacks(context)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1455, in _execute_task_with_callbacks
    result = self._execute_task(context, self.task)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1511, in _execute_task
    result = execute_callable(context=context)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/bash.py", line 188, in execute
    f'Bash command failed. The command returned a non-zero exit code {result.exit_code}.'
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 56.
[2022-02-10 16:35:18,945] {local_task_job.py:154} INFO - Task exited with return code 1
[2022-02-10 16:35:19,010] {local_task_job.py:264} INFO - 0 downstream tasks scheduled from follow-on schedule check
